{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpevasCYc+zt/hxTXT5Zok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/compling/blob/main/projects/faq_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаблон проекта по теме \"Чат-бот для частозадаваемых вопросов\"\n",
        "\n",
        "разработка бота с помощью инструментов NLP для FAQ какой-либо организации (например, НИУ ВШЭ), использовать запросы с большим языковым моделям в работе бота\n",
        "\n",
        "Критерии оценивания\n",
        "\n",
        "- Выгружена база знаний организации, для которой разрабатывается бот (например, произведен парсинг веб-страниц с информацией о НИУ ВШЭ) (2 балла)\n",
        "- База знаний преобразована к виду векторной, реализована функция для поиска семантического сходства (2 балла)\n",
        "- Выполнена функция для генерации ответов на вопросы по информации, извлеченной из базы знаний (2 балла)\n",
        "- Как минимум один запрос успешно выполняется через Telegram-бот (2 балла)\n",
        "- В *.ipynb приложены скриншоты, демонстрирующие работу бота (2 балла)"
      ],
      "metadata": {
        "id": "jESJFTHIJpkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 1\n",
        "\n",
        "Используйте туториалы модуля 2 по веб-скрейпингу, чтобы выгрузить информацию с одной или нескольких страниц для формирования базы знаний вашего бота\n",
        "\n",
        "Уделите внимание сегментации данных - что будет храниться в одном документе?"
      ],
      "metadata": {
        "id": "OBW6RgLnK0Um"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aeoqJt0-_Uw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 2\n",
        "\n",
        "Преобразуйте ваш набор текстов к виду векторной базы данных по образцу"
      ],
      "metadata": {
        "id": "02qDN6f4-Fwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers -q"
      ],
      "metadata": {
        "id": "HaYBCWbQAiX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Загружаем модель (энкодер Транфсормера для формирования векторного пространства)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 1. Кодируем список текстов (документов) из переменной docs с помощью SentenceTransformer\n",
        "embeddings = model.encode(docs, convert_to_numpy=True)\n",
        "\n",
        "# 2. Создаем индекс FAISS (это модель для эмбеддингов)\n",
        "dim = embeddings.shape[1]  # Рассчитываем размерность матрицы эмбеддингов\n",
        "index = faiss.IndexFlatL2(dim)  # Создаем пустую векторную БД нужной размерности dim\n",
        "\n",
        "# Добавляем  embeddings to FAISS index\n",
        "index.add(embeddings)\n",
        "\n",
        "# 3. Делаем запрос к БД: кодируем текущий запрос пользователя\n",
        "embedding = model.encode([text], convert_to_numpy=True)\n",
        "\n",
        "# Производим поиск по БД, ищем k ближайших по вектору документов\n",
        "D, I = index.search(embedding, k=1)\n",
        "\n",
        "# Здесь будет ближайшее по вектору совпадение\n",
        "extracted_sample = docs[I]"
      ],
      "metadata": {
        "id": "XTzJYULn-UyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 3\n",
        "\n",
        "Инициализируйте клиента HuggingFace API\n",
        "\n",
        "Измените системный и пользовательский промпты так, чтобы получилась функция для генерации заданий в формате викторины по темам любой дисциплины на выбор (например, география или история)\n",
        "\n",
        "Используйте образцы:"
      ],
      "metadata": {
        "id": "tX1jwp-IKX-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl3nuO8YW8PG"
      },
      "outputs": [],
      "source": [
        "# Инициализация клиента API\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
        "client = InferenceClient(model_name, token='ваш токен здесь')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание функции для запросов к модели\n",
        "def llm_inference(extracted_sample):\n",
        "  output = client.chat.completions.create(\n",
        "          # Промпты\n",
        "          messages=[\n",
        "              # Основная системная инструкция\n",
        "              {\"role\": \"system\", \"content\": \"you are natural language processing guide\\n\"\n",
        "                                            \"explain the provided topic\\n\"\n",
        "              },\n",
        "              # Пользовательская инструкция: на место extracted_sample\n",
        "              # помещаем информацию, извлеченную из базы знаний\n",
        "              {\"role\": \"user\",\n",
        "              \"content\": f\"explain the basics of {extracted_sample}\"},\n",
        "          ],\n",
        "          stream=False,\n",
        "          max_tokens=128, # Максимальная длина вывода\n",
        "          temperature=0.7, # Температура\n",
        "          top_p=0.1 # Объем выборки для сэмплирования\n",
        "          )\n",
        "  # Вывод текста, ответа модели\n",
        "  return output.choices[0].get('message')['content']"
      ],
      "metadata": {
        "id": "YUENBGthX3cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 4\n",
        "\n",
        "Используйте туториал по работе с aiogram и домашнее задание 3, чтобы разработать функционал Telegram-бота\n",
        "\n",
        "1. Бот принимает на вход вопрос пользователя\n",
        "2. Бот ищет в базе знаний наиболее релевантный ответ\n",
        "3. Бот передает найденный документ языковой модели\n",
        "4. Языковая модель генерирует результат, который выводится ботом"
      ],
      "metadata": {
        "id": "-tyXakHjLOgf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHT5OfyTLjr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 5\n",
        "\n",
        "Вставьте в ячейку ниже скриншоты, которые демонстрируют работу Telegram-бота"
      ],
      "metadata": {
        "id": "WfGECqoTL6vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mDzvaRdywLv3"
      }
    }
  ]
}
