{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV+mtCn7RMzngxGN+pPx9r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/compling/blob/main/projects/teaching_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаблон проекта по теме \"Telegram-бот для изучения иностранного языка\"\n",
        "\n",
        "Критерии оценивания\n",
        "\n",
        "- Выполнена функция для генерации заданий на перевод (2 балла)\n",
        "- Выполнена функция для генерации заданий на грамматику с множественным выбором (2 балла)\n",
        "- Выполнена функция, которая проверяет правильные ответы из JSON-объекта с ответами пользователя (2 балла)\n",
        "- Как минимум одна функция выполняется через Telegram-бот (2 балла)\n",
        "- Функции выполняются в Telegram-боте (в *.ipynb приложены скриншоты, демонстрирующие работу бота) (2 балла)"
      ],
      "metadata": {
        "id": "jESJFTHIJpkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 1\n",
        "\n",
        "Инициализируйте клиента HuggingFace API\n",
        "\n",
        "Измените системный и пользовательский промпты так, чтобы получилась функция для генерации упражнений на отработку навыков перевода для любой языковой пары, например, для перевода с немецкого или французского на русский\n",
        "\n",
        "Используйте образцы:"
      ],
      "metadata": {
        "id": "tX1jwp-IKX-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl3nuO8YW8PG"
      },
      "outputs": [],
      "source": [
        "# Инициализация клиента API\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
        "client = InferenceClient(model_name, token='ваш токен здесь')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание функции для запросов к модели\n",
        "def llm_inference(user_sample):\n",
        "  output = client.chat.completions.create(\n",
        "          # Промпты\n",
        "          messages=[\n",
        "              # Основная системная инструкция\n",
        "              {\"role\": \"system\", \"content\": \"you are natural language processing guide\\n\"\n",
        "                                            \"explain the provided topic\\n\"\n",
        "              },\n",
        "              # Пользовательская инструкция\n",
        "              {\"role\": \"user\",\n",
        "              \"content\": f\"explain the basics of {user_sample}\"},\n",
        "          ],\n",
        "          stream=False,\n",
        "          max_tokens=128, # Максимальная длина вывода\n",
        "          temperature=0.7, # Температура\n",
        "          top_p=0.1 # Объем выборки для сэмплирования\n",
        "          )\n",
        "  # Вывод текста, ответа модели\n",
        "  return output.choices[0].get('message')['content']"
      ],
      "metadata": {
        "id": "YUENBGthX3cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 2\n",
        "\n",
        "Создайте новую функцию для генерации задач на проверку грамматики. Добавьте ограничение на формат ответа\n",
        "\n",
        "Функция должна быть настроена так, чтобы модель производила JSON-объект (словарь dict) следующего вида:\n",
        "\n",
        "```\n",
        "task: \"описание задачи вида \"множественный выбор\" по теме из пользовательского промпта\"\n",
        "\n",
        "solution: \"правильный ответ\"\n",
        "```\n",
        "\n",
        "Используйте сниппеты кода:"
      ],
      "metadata": {
        "id": "OBW6RgLnK0Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Задаем формат ответа\n",
        "response_format = {\n",
        "    \"type\": \"json\",\n",
        "    \"value\": {\n",
        "        \"properties\": {\n",
        "            \"task\": {\"type\": \"string\"},\n",
        "            \"solution\": {\"type\": \"string\"},\n",
        "        },\n",
        "        \"required\": [\"task\", \"solution\",]\n",
        "    },\n",
        "}\n",
        "\n",
        "# Проверяем ответ модели прежде чем писать функцию\n",
        "user_sample = 'Articles in English'\n",
        "# Посмотрите, как мы передаем пользовательский промпт из user_sample\n",
        "# с помощью f-strings форматирования строк\n",
        "response = client.chat_completion(\n",
        "    messages=\n",
        "              # Системные и пользовательские настройки можно изменить\n",
        "              {\"role\": \"system\", \"content\": \"Generate task on the provided topic\"\n",
        "              },\n",
        "              {\"role\": \"user\",\n",
        "              \"content\": f\"Topic: {user_sample}\"},\n",
        "          ],\n",
        "    response_format=response_format,\n",
        "    max_tokens=500,)\n",
        "# Выведем результат: если результат удовлетворительный,\n",
        "# начинаем писать функцию по аналогии с llm_inference из Шага 1\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "id": "X3cF2jwtLLrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После генерации JSON-объект нужно спарсить, т.е. превратить его в настоящий, рабочий словарь"
      ],
      "metadata": {
        "id": "Z1rMFW3lZGZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "# Превращаем сгенерированный JSON (сейчас это строка, похожая на питоновский словарь)\n",
        "# в настоящий питоновский словарь\n",
        "result = ast.literal_eval(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "UDwjtw9rZMIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом примере мы добавили `response_format`, чтобы задать ограничение на генерацию словаря (объекта JSON)\n",
        "\n",
        "Значение `response_format` можно адаптировать, чтобы генерировать условия задач (`task`) и их решения (`solution`)"
      ],
      "metadata": {
        "id": "Z4kc5OZPYpvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 3\n",
        "\n",
        "Напишите функцию, которая принимает на вход ответ пользователя и сравнивает результат со значением ключа `solution` из JSON-объекта, который создается с помощью функции из Шага 2\n",
        "\n",
        "Подсказка:"
      ],
      "metadata": {
        "id": "XnXbRHzdt1dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_user_answer(user_answer, task_dict):\n",
        "    \"\"\"\n",
        "    Сравнивает ответ пользователя с правильным ответом из словаря.\n",
        "\n",
        "    :param user_answer: Ответ пользователя (строка)\n",
        "    :param task_dict: Словарь с задачей и правильным ответом\n",
        "    :return: True, если ответ правильный, иначе False\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Получаем правильный ответ из словаря\n",
        "        correct_answer = task_dict.get('solution')\n",
        "\n",
        "        # Проверяем, что ключ 'solution' существует\n",
        "        if correct_answer is None:\n",
        "            print(\"Ключ 'solution' не найден в словаре.\")\n",
        "            return False\n",
        "\n",
        "        # Сравниваем ответ пользователя с правильным ответом\n",
        "        return user_answer == correct_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "w7Qi4jSdubFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Замените вывод `True/False` на вывод сообщений для пользователя\n",
        "\n",
        "Добавьте вывод правильного варианта ответа, если пользователь ответил неправильно\n",
        "\n",
        "Протестируйте функцию на сгенерированных задачах из Шага 2"
      ],
      "metadata": {
        "id": "3vRTX8DvvqX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 4\n",
        "\n",
        "Используйте туториал по работе с aiogram и домашнее задание 3, чтобы разработать функционал Telegram-бота\n",
        "\n",
        "Для зачета проекта достаточно, чтобы чат-бот предлагал пользователю задание. Проверку решений можно реализовать *по желанию*. Это не повлияет на оценку\n",
        "\n",
        "Добавьте 2 кнопки: \"упражнения на перевод\" и \"задачи на грамматику\""
      ],
      "metadata": {
        "id": "-tyXakHjLOgf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHT5OfyTLjr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 4\n",
        "\n",
        "Примените к столбцам с исходными метками и метками NLTK методы для расчета метрик Precision, Recall, F-Score, чтобы оценить качество работы NLTK\n",
        "\n",
        "Используйте сниппеты кода:"
      ],
      "metadata": {
        "id": "ogYMhQtnLxH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Расчет метрик оценки\n",
        "precision = precision_score(df['label'], df['predicted_label'])\n",
        "recall = recall_score(df['label'], df['predicted_label'])\n",
        "f1 = f1_score(df['label'], df['predicted_label'])"
      ],
      "metadata": {
        "id": "NhmqiuIUUkXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуализируйте результат с использованием матрицы ошибок\n",
        "\n",
        "Для визуализации используйте heatmap из библиотеки seaborn\n",
        "\n",
        "https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ],
      "metadata": {
        "id": "D1VMigHRemDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Расчет матрицы ошибок\n",
        "conf_matrix = confusion_matrix(df['label'], df['predicted_label'])"
      ],
      "metadata": {
        "id": "hbBpBd2Mesaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Шаг 5\n",
        "\n",
        "Вставьте в ячейку ниже скриншоты, которые демонстрируют работу Telegram-бота"
      ],
      "metadata": {
        "id": "WfGECqoTL6vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mDzvaRdywLv3"
      }
    }
  ]
}