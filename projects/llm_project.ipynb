{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPlIY8HIvFVk0wYsgVaUm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/compling/blob/main/projects/llm_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Псевдокод для проектов с LLM"
      ],
      "metadata": {
        "id": "PIbIC9dmXIA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. инициализация клиента API"
      ],
      "metadata": {
        "id": "IIbLbK0DXuEA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl3nuO8YW8PG"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\""
      ],
      "metadata": {
        "id": "ulhGD2YjXQMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = InferenceClient(model_name, token='ваш токен здесь')"
      ],
      "metadata": {
        "id": "FJJKYPeQXo_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. создание функции для запросов к модели"
      ],
      "metadata": {
        "id": "0A0M_9jMXsz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_inference(user_sample):\n",
        "  output = client.chat.completions.create(\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": \"you are natural language processing guide\\n\"\n",
        "                                            \"explain the provided topic\\n\"\n",
        "              },\n",
        "              {\"role\": \"user\",\n",
        "              \"content\": f\"explain the basics of {user_sample}\"},\n",
        "          ],\n",
        "          stream=False,\n",
        "          max_tokens=128,\n",
        "          temperature=0.7,\n",
        "          top_p=0.1\n",
        "          )\n",
        "  return output.choices[0].get('message')['content']"
      ],
      "metadata": {
        "id": "YUENBGthX3cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примеры функций:\n",
        "\n",
        "- функция с промптом для генерации случайных текстов, которые нужно перевести\n",
        "- функция принимает на вход задачу на перевод текста и перевод --> используем промпт для оценки качества перевода\n",
        "- функция принимает на вход описание темы для генерации задач на грамматику со множественным выбором"
      ],
      "metadata": {
        "id": "UqzybrYNX5S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. создание функции с возможностью парсинга словаря (json-объекта)"
      ],
      "metadata": {
        "id": "gHeCkmMlYZVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# формат ответа\n",
        "response_format = {\n",
        "    \"type\": \"json\",\n",
        "    \"value\": {\n",
        "        \"properties\": {\n",
        "            \"concept_name\": {\"type\": \"string\"},\n",
        "            \"concept_description\": {\"type\": \"string\"},\n",
        "        },\n",
        "        \"required\": [\"concept_name\", \"concept_description\",]\n",
        "    },\n",
        "}\n",
        "\n",
        "# ответ\n",
        "response = client.chat_completion(\n",
        "    messages=\n",
        "              {\"role\": \"system\", \"content\": \"you are natural language processing guide\\n\"\n",
        "                                            \"explain the provided topic\\n\"\n",
        "              },\n",
        "              {\"role\": \"user\",\n",
        "              \"content\": f\"explain the basics of {user_sample}\"},\n",
        "          ],\n",
        "    response_format=response_format,\n",
        "    max_tokens=500,)\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "id": "uoc76iGjYXaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После генерации json ответ нужно спарсить, превратить его в настоящий, рабочий словарь"
      ],
      "metadata": {
        "id": "Z1rMFW3lZGZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "result = ast.literal_eval(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "UDwjtw9rZMIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом примере мы добавили `response_format`, чтобы задать ограничение на генерацию словаря (объекта json)\n",
        "\n",
        "`response_format` можно адаптировать, чтобы генерировать условия задач и их решения (варианты ответов)"
      ],
      "metadata": {
        "id": "Z4kc5OZPYpvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. разработка функций для проверки ответов пользователя\n",
        "\n",
        "- здесь вы можете использовать средства нативного Python (например, условия и циклы), чтобы проверить, совпал ли ответ пользователя с правильным ответом из объекта json"
      ],
      "metadata": {
        "id": "f5JzFsMIY3Pg"
      }
    }
  ]
}